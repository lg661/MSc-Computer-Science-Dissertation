{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_GA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# BELOW IS CODE ADAPTED FROM:\n",
        "# \n",
        "# Escape Velocity Labs, 28/08/22, 'Advanced Reinforcement Learning in Python: from DQN to SAC', \n",
        "# Accessed at https://www.udemy.com/course/advanced-reinforcement/ \n",
        "#\n",
        "# Note: There is a section towards the bottom which is purely self coded (all except for the Training function used), this has been labelled clearly.\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "oP4hiMj6g1Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILh7zuewiVV"
      },
      "outputs": [],
      "source": [
        "### Install dependencies\n",
        "### Note: you need to restart the runtime after you run this cell\n",
        "\n",
        "!apt-get install -y xvfb # Used to display videos and GUI's in this notebook\n",
        "\n",
        "!pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d] \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyvirtualdisplay\n",
        "\n",
        "# gym -> Environment \n",
        "# gym[box2d] -> Dependency for box2d category in the gym\n",
        "# pytorch-lightning -> Reinforcement learning and neural networks\n",
        "# pyvirtualdisplay -> Used to display videos in the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a display to render the environment in google servers\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "PpxHpXkoxZ8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Import remaining packages and set up variables related to hardware\n",
        "\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "# Use GPU if there is one available\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_gpus = torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "YaXJy_vRxanw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function which takes episode number as input, and outputs a video of the environment of the given episode\n",
        "\n",
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ],
      "metadata": {
        "id": "29cLrpumxawF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the actor network class\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, min, max, num_layers):\n",
        "    super().__init__()\n",
        "    self.min = torch.from_numpy(min).to(device)\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "    if num_layers == 1:\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(obs_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, out_dims),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "    elif num_layers == 2:\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(obs_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, out_dims),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "    elif num_layers == 3:\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(obs_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, out_dims),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "    \n",
        "  # This function takes a state as input and outputs the highest return action\n",
        "  def mu(self, x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "    return self.net(x.float()) * self.max\n",
        "\n",
        "  # This function takes a state as input, and returns the highest return action with clipped noise\n",
        "  def forward(self, x, epsilon=0.0, noise_clip=None):\n",
        "    mu = self.mu(x)\n",
        "    noise = torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "    if noise_clip is not None:\n",
        "      noise = torch.clamp(noise, -noise_clip, noise_clip)\n",
        "    mu = mu + noise\n",
        "    action = torch.max(torch.min(mu, self.max), self.min)\n",
        "    action = action.detach().cpu().numpy() # Maybe not detach\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "AlrfMIGb1Z99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the Q-Network class\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, num_layers):\n",
        "    super().__init__()\n",
        "    if num_layers == 1:\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(obs_size + out_dims, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),           \n",
        "          nn.Linear(hidden_size, 1),\n",
        "      )\n",
        "    elif num_layers == 2:\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(obs_size + out_dims, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),                \n",
        "          nn.Linear(hidden_size, 1),\n",
        "      )\n",
        "    elif num_layers == 3:\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + out_dims, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),     \n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),                \n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "  # This function takes a state-action pair and returns a Q-value (predicted expected return)\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())\n"
      ],
      "metadata": {
        "id": "jh6Cv7LYxa7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the ReplayBuffer class, from which we make out buffer\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "Nsr6T3zuxbJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the RLDataset class, which is used to create a dataset object out of samples from the buffer to be used as input for the training step\n",
        "\n",
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=200):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  # This function sequentially gives an experience from the buffer to the pyTorch when requested\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield  experience\n"
      ],
      "metadata": {
        "id": "-dZypud4xbQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function to incrementally move the weights of the target network closer to those of the main network\n",
        "\n",
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ],
      "metadata": {
        "id": "_jULN7Nx2xBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function which wraps the default environment from gym in layers of additional funcitonality\n",
        "\n",
        "def create_environment(name, max_steps):\n",
        "  env = gym.make(name)\n",
        "  env = TimeLimit(env, max_episode_steps=max_steps)\n",
        "  #env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "LmR2lG_9xbV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This class enacts the TD3 algorithm\n",
        "\n",
        "class TD3(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=50_000, batch_size=100, \n",
        "               actor_lr=1e-3, critic_lr=1e-3, hidden_size=128, gamma=0.995,\n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.1,\n",
        "               eps_last_episode=100, samples_per_epoch=1_000, tau=0.004, epsilon=1, max_steps=400, num_layers=1):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name, max_steps)\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "    min_action = self.env.action_space.low\n",
        "\n",
        "    self.q_net1 = DQN(hidden_size, obs_size, action_dims, num_layers)\n",
        "    self.q_net2 = DQN(hidden_size, obs_size, action_dims, num_layers)\n",
        "    self.actor = Actor(hidden_size, obs_size, action_dims, min_action, max_action, num_layers)\n",
        "\n",
        "    self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "    self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "    self.target_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.ep_returns = []\n",
        "\n",
        "    self.max_return = 0\n",
        "    self.best_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    # Fill the buffer up with experience (1000 experiences - i.e. self.hparams.samples_per_epoch)\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  # This function plays a single full episode and stores all of the experience in the buffer\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, actor=None, epsilon=0.):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if actor:\n",
        "        action = actor(state, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      self.buffer.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "  # This function performs returns a predicted high returning action with noise applied\n",
        "  def forward(self, x):\n",
        "    output = self.actor(x)\n",
        "    return output\n",
        "  \n",
        "  # This function configures the optimizers before the process begins\n",
        "  def configure_optimizers(self):\n",
        "    q_net_params = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "    q_net_optimizer = self.hparams.optim(q_net_params, lr=self.hparams.critic_lr)\n",
        "    actor_optimizer = self.hparams.optim(self.actor.parameters(), lr=self.hparams.actor_lr)\n",
        "    return [q_net_optimizer, actor_optimizer]\n",
        "  \n",
        "  # This function creates the dataloader, which feeds experience to the training_step() function.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size = self.hparams.batch_size\n",
        "    )\n",
        "    return dataloader\n",
        "  \n",
        "  # Training step to update the neural networks\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    # Update the target networks\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "    polyak_average(self.actor, self.target_actor, tau=self.hparams.tau)\n",
        "\n",
        "    # If the q_net optimizer is selected, update the Q-network\n",
        "    if optimizer_idx == 0:\n",
        "      state_action_values1 = self.q_net1(states, actions)\n",
        "      state_action_values2 = self.q_net2(states, actions)\n",
        "      next_actions = self.target_actor(next_states, epsilon=self.epsilon, noise_clip=0.05)\n",
        "      # Select the lowest next state/action value\n",
        "      next_state_action_values = torch.min(\n",
        "          self.target_q_net1(next_states, next_actions),\n",
        "          self.target_q_net2(next_states, next_actions)\n",
        "      )\n",
        "      # Set all terminal states to have a value of 0\n",
        "      next_state_action_values[dones] = 0.0\n",
        "      expected_state_action_values = rewards + self.hparams.gamma * next_state_action_values\n",
        "\n",
        "      q_loss1 = self.hparams.loss_fn(state_action_values1, expected_state_action_values)\n",
        "      q_loss2 = self.hparams.loss_fn(state_action_values2, expected_state_action_values)\n",
        "      total_loss = q_loss1 + q_loss2\n",
        "      self.log_dict({\"episode/Q-Loss\": total_loss})\n",
        "      return total_loss\n",
        "\n",
        "    # If the actor optimizer is selected, update the Actor network\n",
        "    elif optimizer_idx == 1 and batch_idx % 2 == 0:\n",
        "      mu = self.actor.mu(states)\n",
        "      actor_loss = - self.q_net1(states, mu).mean()\n",
        "      self.log_dict({\"episode/Actor Loss\": actor_loss})\n",
        "      return actor_loss\n",
        "\n",
        "\n",
        "  # This function is called at the end of each epoch\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    # Decrease epsilon value as time goes on to reduce exploration and increase reward\n",
        "    self.epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    # An episode is played out in full at the end of each epoch, therefore one epoch lasts as long as one episode\n",
        "    self.play_episode(actor=self.actor, epsilon=self.epsilon)\n",
        "    self.log('episode/Return', self.env.return_queue[-1])\n",
        "\n",
        "    self.ep_returns.append(self.env.return_queue[-1])\n",
        "\n",
        "    if self.env.return_queue[-1] > self.max_return:\n",
        "      self.best_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wAZUzridxbb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up the tensorboard to display the results\n",
        "\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ],
      "metadata": {
        "id": "hvHu6HAZxyw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "BELOW IS MY CODE FOR HPS-RL\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "yxvb5p0ShE7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### The fitness function of the GA algorithm\n",
        "\n",
        "def fitness_prob(algoA_actor, algoB_actor, algoC_actor, algoD_actor, num_tests, gene_A, gene_B, gene_C, gene_D, gen):\n",
        "  env_name = 'BipedalWalker-v3'\n",
        "  max_steps = 1600\n",
        "  env = create_environment(env_name, max_steps)\n",
        "  actors_list = [algoA_actor, algoB_actor, algoC_actor, algoD_actor]\n",
        "  genes_list = [gene_A, gene_B, gene_C, gene_D]\n",
        "  la_test_returns = []\n",
        "\n",
        "  # Run the 100 tests with the last actors of each algorithm\n",
        "  # Create the list la_test_returns = [[results of algoA eps 1-100], [results of algoB eps 1-100], [results of algoC eps 1-100]]\n",
        "  for i, actor in enumerate(actors_list):\n",
        "    la_test_returns.append([])\n",
        "    for test in range(num_tests):\n",
        "      state = env.reset()\n",
        "      ep_return = 0\n",
        "      done = False\n",
        "      while not done:\n",
        "        action = actor(state, epsilon=0)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "        state = next_state\n",
        "        ep_return += reward\n",
        "      la_test_returns[i].append(ep_return)\n",
        "    \n",
        "    # Store the results of the test\n",
        "    this_Algo = pd.DataFrame(la_test_returns[i])\n",
        "    this_Algo.columns = ['Gene: ' +  f'{genes_list[i]}']\n",
        "    if i==0:\n",
        "      results = this_Algo\n",
        "    else:\n",
        "      results = pd.concat([results, this_Algo], axis=1)\n",
        "  results.to_csv(f'TD3_GA_Generation_{gen}_fitness_test.csv')\n",
        "\n",
        "  \n",
        "  # Calculate the fitnesses of each algorithm (*note: ignoring loss at this stage - unclear what it is + appears it should work without also)\n",
        "  # Add 150 to each score to work with any negative numbers\n",
        "  algoA_fitness = (1/num_tests) + sum(la_test_returns[0]) + 150*num_tests\n",
        "  algoB_fitness = (1/num_tests) + sum(la_test_returns[1]) + 150*num_tests\n",
        "  algoC_fitness = (1/num_tests) + sum(la_test_returns[2]) + 150*num_tests\n",
        "  algoD_fitness = (1/num_tests) + sum(la_test_returns[3]) + 150*num_tests\n",
        "\n",
        "  # If fitness still < 0 despite + 100\n",
        "  fitness_list = []\n",
        "  for fitness in [algoA_fitness, algoB_fitness, algoC_fitness, algoD_fitness]:\n",
        "    if fitness < 0:\n",
        "      fitness = 0\n",
        "    fitness_list.append(fitness)\n",
        "\n",
        "  fitness_total = sum(fitness_list)\n",
        "\n",
        "  algoA_prob = fitness_list[0]/fitness_total\n",
        "  algoB_prob = fitness_list[1]/fitness_total\n",
        "  algoC_prob = fitness_list[2]/fitness_total\n",
        "  algoD_prob = fitness_list[3]/fitness_total\n",
        "\n",
        "  return (algoA_prob,algoB_prob, algoC_prob, algoD_prob, la_test_returns)"
      ],
      "metadata": {
        "id": "R-E9xgIzolgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Randomly select a gene by considering its fitness\n",
        "\n",
        "def randomly_select(gene_A, algoA_prob, gene_B, algoB_prob, gene_C, algoC_prob, gene_D, algoD_prob):\n",
        "  # Declare parents\n",
        "  parent_11 = [0,0]\n",
        "  parent_22 = [0,0]\n",
        "  # Get parent 1\n",
        "  rand_1 = random.uniform(0, 1)\n",
        "  if rand_1 <= algoA_prob:\n",
        "    parent_11 = gene_A\n",
        "  elif rand_1 <= algoA_prob + algoB_prob:\n",
        "    parent_11 = gene_B\n",
        "  elif rand_1 <= algoA_prob + algoB_prob + algoC_prob:\n",
        "    parent_11 = gene_C\n",
        "  elif rand_1 <= algoA_prob + algoB_prob + algoC_prob + algoD_prob:\n",
        "    parent_11 = gene_D\n",
        "\n",
        "  # Get parent 2\n",
        "  parent_22 = copy.deepcopy(parent_11)\n",
        "  while parent_22 == parent_11:\n",
        "    rand_2 = random.uniform(0, 1)\n",
        "    if rand_2 <= algoA_prob:\n",
        "      parent_22 = gene_A\n",
        "    elif rand_2 <= algoA_prob + algoB_prob:\n",
        "      parent_22 = gene_B\n",
        "    elif rand_2 <= algoA_prob + algoB_prob + algoC_prob:\n",
        "      parent_22 = gene_C\n",
        "    elif rand_2 <= algoA_prob + algoB_prob + algoC_prob + algoD_prob:\n",
        "      parent_22 = gene_D\n",
        "\n",
        "  return (parent_11, parent_22)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "CSV3yzTe0KKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Perform crossover\n",
        "\n",
        "def crossover(parent_11, parent_22):\n",
        "  child_11 = copy.deepcopy(parent_11)\n",
        "  child_22 = copy.deepcopy(parent_22)\n",
        "  # Select which hyperparameter to switch\n",
        "  index = random.choice([0, 1])\n",
        "  if index == 0:\n",
        "    child_11 = (parent_22[0], parent_11[1])\n",
        "    child_22 = (parent_11[0], parent_22[1])\n",
        "  elif index == 1:\n",
        "    child_11 = (parent_11[0], parent_22[1])\n",
        "    child_22 = (parent_22[0], parent_11[1])\n",
        "  return (child_11, child_22)\n",
        "  \n"
      ],
      "metadata": {
        "id": "4y0hwUcl6jEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Perform mutation\n",
        "\n",
        "def mutate(child_11, child_22, max_hidden_size, min_hidden_size, max_num_layers, min_num_layers):\n",
        "  # Declare children\n",
        "  child_33 = (0, 0)\n",
        "  child_44 = (0, 0)\n",
        "  # Select which hyperparameter to mutate for child_3\n",
        "  index_1 = random.choice([0, 1])\n",
        "  if index_1 == 0:\n",
        "    child_33 = (random.randint(min_hidden_size, max_hidden_size), child_11[1])\n",
        "  elif index_1 == 1:\n",
        "    child_33 = (child_11[0], random.randint(min_num_layers, max_num_layers))\n",
        "\n",
        "  # Select which hyperparameter to mutate for child_4\n",
        "  index_2 = random.choice([0, 1])\n",
        "  if index_2 == 0:\n",
        "    child_44 = (random.randint(min_hidden_size, max_hidden_size), child_22[1])\n",
        "  elif index_2 == 1:\n",
        "    child_44 = (child_22[0], random.randint(min_num_layers, max_num_layers))\n",
        "\n",
        "  return (child_33, child_44)\n"
      ],
      "metadata": {
        "id": "PXTM3M5Z6kpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Start the GA process\n",
        "\n",
        "# Set values\n",
        "num_gens = 13\n",
        "num_tests = 100\n",
        "eps_per_it = 3000\n",
        "fitness_list = []\n",
        "\n",
        "# Initialize genes, GA parameters, and set limits\n",
        "max_hidden_size = 500\n",
        "min_hidden_size = 50\n",
        "\n",
        "max_num_layers = 3\n",
        "min_num_layers = 1\n",
        "\n",
        "gene_A = (500, 1)\n",
        "gene_B = (250, 2)\n",
        "gene_C = (50, 3)\n",
        "gene_D = (128, 1)\n",
        "gene_history = [gene_A,  gene_B, gene_C, gene_D]\n",
        "\n",
        "\n",
        "for gen in range(num_gens):\n",
        "  \n",
        "  # Reset returns lists\n",
        "  gene_A_returns = []\n",
        "  gene_B_returns = []\n",
        "  gene_C_returns = []\n",
        "  gene_D_returns = []\n",
        "\n",
        "  # Train the models with gene A\n",
        "  algoA = TD3('BipedalWalker-v3', hidden_size=gene_A[0], num_layers=gene_A[1])\n",
        "  trainer = Trainer(\n",
        "      gpus=num_gpus,\n",
        "      max_epochs=eps_per_it\n",
        "  )\n",
        "  trainer.fit(algoA)\n",
        "  gene_A_returns.append(algoA.ep_returns)\n",
        "\n",
        "  # Train the models with gene B\n",
        "  algoB = TD3('BipedalWalker-v3', hidden_size=gene_B[0], num_layers=gene_B[1])\n",
        "  trainer = Trainer(\n",
        "      gpus=num_gpus,\n",
        "      max_epochs=eps_per_it\n",
        "  )\n",
        "  trainer.fit(algoB)\n",
        "  gene_B_returns.append(algoB.ep_returns)\n",
        "\n",
        "  # Train the models with gene C\n",
        "  algoC = TD3('BipedalWalker-v3', hidden_size=gene_C[0], num_layers=gene_C[1])\n",
        "  trainer = Trainer(\n",
        "      gpus=num_gpus,\n",
        "      max_epochs=eps_per_it\n",
        "  )\n",
        "  trainer.fit(algoC)\n",
        "  gene_C_returns.append(algoC.ep_returns)\n",
        "\n",
        "  # Train the models with gene D\n",
        "  algoD = TD3('BipedalWalker-v3', hidden_size=gene_D[0], num_layers=gene_D[1])\n",
        "  trainer = Trainer(\n",
        "      gpus=num_gpus,\n",
        "      max_epochs=eps_per_it\n",
        "  )\n",
        "  trainer.fit(algoD)\n",
        "  gene_D_returns.append(algoD.ep_returns)\n",
        "\n",
        "  # Test the fitness of the models\n",
        "  algoA_prob, algoB_prob, algoC_prob, algoD_prob, la_test_returns  = fitness_prob(algoA.actor, algoB.actor, algoC.actor, algoD.actor, num_tests, gene_A, gene_B, gene_C, gene_D, gen)\n",
        "  fitness_list.append([algoA_prob, algoB_prob, algoC_prob, algoD_prob])\n",
        "\n",
        "  # Store the results of the fitness of the models\n",
        "  this_Algo = pd.DataFrame([algoA_prob, algoB_prob, algoC_prob, algoD_prob])\n",
        "  this_Algo.columns = ['Algo generation: ' +  f'{gen}']\n",
        "  if gen==0:\n",
        "    fitness_results = this_Algo\n",
        "  else:\n",
        "    fitness_results = pd.concat([fitness_results, this_Algo], axis=1)\n",
        "  \n",
        "  # Select the next parents\n",
        "  parent_1, parent_2 = randomly_select(gene_A, algoA_prob, gene_B, algoB_prob, gene_C, algoC_prob, gene_D, algoD_prob)\n",
        "\n",
        "  # Perform crossover\n",
        "  child_1, child_2 = crossover(copy.deepcopy(parent_1), copy.deepcopy(parent_2))\n",
        "\n",
        "  # Perform mutation\n",
        "  child_3, child_4 = mutate(copy.deepcopy(child_1), copy.deepcopy(child_2), max_hidden_size, min_hidden_size, max_num_layers, min_num_layers)\n",
        "\n",
        "  # Create excel file for the last 4 algorithms episode returns\n",
        "  algo_A_df = pd.DataFrame(gene_A_returns[0])\n",
        "  algo_A_df.columns = ['Algo generation: ' +  f'{gen}' + ', gene: ' + f'{gene_A}']\n",
        "  algo_A_df.to_csv(f'TD3_GA_returns_gen_{gen}_gene_[{gene_A[0]},{gene_A[1]}].csv')\n",
        "  algo_B_df = pd.DataFrame(gene_B_returns[0])\n",
        "  algo_B_df.columns = ['Algo generation: ' +  f'{gen}' + ', gene: ' + f'{gene_B}']\n",
        "  algo_B_df.to_csv(f'TD3_GA_returns_gen_{gen}_gene_[{gene_B[0]},{gene_B[1]}].csv')\n",
        "  algo_C_df = pd.DataFrame(gene_C_returns[0])\n",
        "  algo_C_df.columns = ['Algo generation: ' +  f'{gen}' + ', gene: ' + f'{gene_C}']\n",
        "  algo_C_df.to_csv(f'TD3_GA_returns_gen_{gen}_gene_[{gene_C[0]},{gene_C[1]}].csv')\n",
        "  algo_D_df = pd.DataFrame(gene_D_returns[0])\n",
        "  algo_D_df.columns = ['Algo generation: ' +  f'{gen}' + ', gene: ' + f'{gene_D}']\n",
        "  algo_D_df.to_csv(f'TD3_GA_returns_gen_{gen}_gene_[{gene_D[0]},{gene_D[1]}].csv')\n",
        "\n",
        "  print(f\"Generation: {gen}, Gene [{gene_A[0]},{gene_A[1]}], mean 100 ep score = {sum(la_test_returns[0])/num_tests}, probability = {algoA_prob}\")\n",
        "  print(f\"Generation: {gen}, Gene [{gene_B[0]},{gene_B[1]}], mean 100 ep score = {sum(la_test_returns[1])/num_tests}, probability = {algoB_prob}\")\n",
        "  print(f\"Generation: {gen}, Gene [{gene_C[0]},{gene_C[1]}], mean 100 ep score = {sum(la_test_returns[2])/num_tests}, probability = {algoC_prob}\")\n",
        "  print(f\"Generation: {gen}, Gene [{gene_D[0]},{gene_D[1]}], mean 100 ep score = {sum(la_test_returns[3])/num_tests}, probability = {algoD_prob}\")\n",
        "\n",
        "  # Set the genes for the next round of training\n",
        "  gene_A = copy.deepcopy(child_1)\n",
        "  gene_B = copy.deepcopy(child_2)\n",
        "  gene_C = copy.deepcopy(child_3)\n",
        "  gene_D = copy.deepcopy(child_4)\n",
        "\n",
        "  print(f\"Next genes are: [{gene_A[0]},{gene_A[1]}] + [{gene_B[0]},{gene_B[1]}] + [{gene_C[0]},{gene_C[1]}] + [{gene_D[0]},{gene_D[1]}]\")\n",
        "\n",
        "# Create excel file for GA gens fitness results\n",
        "fitness_results.to_csv(f'TD3_GA_fitness_generation.csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_PutbQJDodiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJncMXPjxy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXpp8F-PxzCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}