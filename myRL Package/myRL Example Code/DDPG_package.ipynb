{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_package.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### Create a display to render the environment in google servers\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "Y-Erq_AyQAzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Import remaining packages and set up variables related to hardware\n",
        "\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tabulate import tabulate\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "# Use GPU if there is one available\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_gpus = torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "iQyWzx1lrYyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function which takes episode number as input, and outputs a video of the environment of the given episode\n",
        "\n",
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ],
      "metadata": {
        "id": "8s48kbnMQt8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the actor network class\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "    super().__init__()\n",
        "    self.min = torch.from_numpy(min).to(device)\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, out_dims),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  # This function takes a state as input and outputs the highest return action\n",
        "  def mu(self, x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "    return self.net(x.float()) * self.max\n",
        "\n",
        "  # This function takes a state as input, and returns the highest return action with noise\n",
        "  def forward(self, x, epsilon=0.0):\n",
        "    mu = self.mu(x)\n",
        "    mu = mu + torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "    action = torch.max(torch.min(mu, self.max), self.min)\n",
        "    action = action.detach().cpu().numpy()\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "fFRkpUIZTJS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the Q-Network class\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),           \n",
        "        nn.Linear(hidden_size, 1),\n",
        "    )\n",
        "\n",
        "  # This function takes a state-action pair and returns a Q-value (predicted expected return)\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())\n"
      ],
      "metadata": {
        "id": "iCsybYo3TJVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the ReplayBuffer class, from which we make out buffer\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "FKZ5BMsHTJYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the RLDataset class, which is used to create a dataset object out of samples from the buffer to be used as input for the training step\n",
        "\n",
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=200):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  # This function sequentially gives an experience from the buffer to the pyTorch when requested\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield  experience\n"
      ],
      "metadata": {
        "id": "sYnGoXVVTJbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function to incrementally move the weights of the target network closer to those of the main network\n",
        "\n",
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ],
      "metadata": {
        "id": "p3F1LtUATJdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a function which wraps the default environment from gym in layers of additional funcitonality\n",
        "\n",
        "def create_environment(name, max_steps):\n",
        "  env = gym.make(name)\n",
        "  env = TimeLimit(env, max_episode_steps=max_steps)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "mX71zyAZTJjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This class enacts the DDPG algorithm\n",
        "\n",
        "class DDPG(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=300_000,\n",
        "               batch_size=200, actor_lr=1e-3, critic_lr=0.0002, hidden_size=288, gamma=0.99,\n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n",
        "               eps_last_episode=450, samples_per_epoch=1_000, tau=0.01, max_steps=400):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name, max_steps)\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "    min_action = self.env.action_space.low\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, action_dims)\n",
        "    self.actor = Actor(hidden_size, obs_size, action_dims, min_action, max_action)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "    self.target_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.episodic_rewards = []\n",
        "    self.agent = copy.deepcopy(self.actor)\n",
        "\n",
        "    self.max_return = 0\n",
        "    self.best_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    # Fill the buffer up with experience (1000 experiences - i.e. self.hparams.samples_per_epoch)\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  # This function plays a single full episode and stores all of the experience in the buffer\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, actor=None, epsilon=0.):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if actor:\n",
        "        action = actor(state, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      self.buffer.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "  # This function performs returns a predicted high returning action with noise applied\n",
        "  def forward(self, x):\n",
        "    output = self.actor(x)\n",
        "    return output\n",
        "  \n",
        "  # This function configures the optimizers before the process begins\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.critic_lr)\n",
        "    actor_optimizer = self.hparams.optim(self.actor.parameters(), lr=self.hparams.actor_lr)\n",
        "    return [q_net_optimizer, actor_optimizer]\n",
        "  \n",
        "  # This function creates the dataloader, which feeds experience to the training_step() function.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size = self.hparams.batch_size\n",
        "    )\n",
        "    return dataloader\n",
        "  \n",
        "  # Training step to update the neural networks\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    # Update the target networks\n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    polyak_average(self.actor, self.target_actor, tau=self.hparams.tau)\n",
        "\n",
        "    # If the q_net optimizer is selected, update the Q-network\n",
        "    if optimizer_idx == 0:\n",
        "      state_action_values = self.q_net(states, actions)\n",
        "      next_state_values = self.target_q_net(next_states, self.target_actor.mu(next_states))\n",
        "      # Set all terminal states to have a value of 0\n",
        "      next_state_values[dones] = 0.0\n",
        "      expected_state_action_values = rewards + self.hparams.gamma * next_state_values\n",
        "      q_loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
        "      self.log_dict({\"episode/Q-Loss\": q_loss})\n",
        "      return q_loss\n",
        "    \n",
        "    # If the actor optimizer is selected, update the Actor network\n",
        "    elif optimizer_idx == 1:\n",
        "      mu = self.actor.mu(states)\n",
        "      actor_loss = - self.q_net(states, mu).mean()\n",
        "      self.log_dict({\"episode/Actor Loss\": actor_loss})\n",
        "      return actor_loss\n",
        "\n",
        "  # This function is called at the end of each epoch\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    # Decrease epsilon value as time goes on to reduce exploration and increase reward\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    # An episode is played out in full at the end of each epoch, therefore one epoch lasts as long as one episode\n",
        "    self.play_episode(actor=self.actor, epsilon=epsilon)\n",
        "    self.log('episode/Return', self.env.return_queue[-1])\n",
        "    self.episodic_rewards.append(self.env.return_queue[-1])\n",
        "\n",
        "    # If the bettter than best actor, save as best actor\n",
        "    if self.env.return_queue[-1] > self.max_return:\n",
        "      self.best_actor = copy.deepcopy(self.actor)\n",
        "      self.max_return = self.env.return_queue[-1]\n",
        "\n",
        "    self.agent = copy.deepcopy(self.actor)\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "KZDhds_ETJkU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}